{"cells":[{"cell_type":"markdown","id":"31155cfe-d347-4bfd-aa69-dcb6204185c2","metadata":{"id":"31155cfe-d347-4bfd-aa69-dcb6204185c2"},"source":["## Refer to paper, codebase and google doc you have"]},{"cell_type":"markdown","id":"ea8c4bdb-9b62-4bae-8642-878f095cb91b","metadata":{"id":"ea8c4bdb-9b62-4bae-8642-878f095cb91b"},"source":["## Experiment 1: Refer to Fig. 2 and Appendix Fig. 7. Use 105 features of Magnifier"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","from sklearn.ensemble import IsolationForest\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report,accuracy_score, confusion_matrix, roc_auc_score, f1_score\n","from sklearn.metrics import roc_curve, auc, precision_recall_curve, classification_report, average_precision_score\n","import torch\n","from torch import nn, optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from fcntl import F_SETFL\n","import pickle\n","import time\n","import random\n","from queue import Queue\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","criterion = nn.MSELoss()\n","scaler = preprocessing.MinMaxScaler()"],"metadata":{"id":"leNL3eUBJhRy"},"id":"leNL3eUBJhRy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Returning all files in directory.\n","def file_name_walk(file_dir):\n","    file_list = []\n","    for root, dirs, files in os.walk(file_dir):\n","        for file in files:\n","            if os.path.splitext(file)[1] == \".csv\":\n","                file_list.append(\"{}/{}\".format(root, file))\n","    return file_list"],"metadata":{"id":"khuQcVWdJzM5"},"id":"khuQcVWdJzM5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading benign data from a list of devices.\n","def load_iot_data_seq(device_list=['philips_camera'],begin=0,end=5):\n","    df_normal = pd.DataFrame()\n","    normal_list = device_list\n","    for type_index, type_name in enumerate(normal_list):\n","        file_list = file_name_walk(\n","            './DataSets/normal-kitsune_test/{:}'.format(type_name))\n","        file_list.sort()\n","        df_normal_type = pd.DataFrame()\n","        begin_num = begin\n","        end_num = end\n","        for i, file_path in enumerate(file_list[begin_num:end_num]):\n","            try:\n","                tmp_df = pd.read_csv(file_path, header=None)\n","                df_normal_type = pd.concat([df_normal_type,tmp_df], ignore_index=True)\n","                #df_normal_type = df_normal_type.append(tmp_df, ignore_index=True)\n","            except:\n","                print(file_path)\n","        df_normal = pd.concat([df_normal,df_normal_type], ignore_index=True)\n","        #df_normal = df_normal.append(df_normal_type, ignore_index=True)\n","    df_normal['class'] = 0\n","    df_normal.fillna(0,inplace=True)\n","    return df_normal"],"metadata":{"id":"2lgISN1mJ1Kt"},"id":"2lgISN1mJ1Kt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading malicious data from a list of attacks.\n","def load_iot_attack_seq(attack_name='all'):\n","    df_attack = pd.DataFrame()\n","    df_attack_label = pd.DataFrame()\n","    attack_path = './DataSets/Anomaly/attack_kitsune/'\n","    if (attack_name=='all'):\n","        attack_list=os.listdir(attack_path)\n","    else:\n","        attack_list = [attack_name]\n","    for type_index, type_name in enumerate(attack_list):\n","        file_list = file_name_walk('./DataSets/Anomaly/attack_kitsune/{:}'.format(type_name))\n","        for i, file_path in enumerate(file_list):\n","            if 'label' in file_path:\n","                continue\n","            tmp_df = pd.read_csv(file_path, header=None)\n","            df_attack = pd.concat([df_attack, tmp_df], ignore_index=True)\n","            if file_path.replace('.csv', '_label.csv') in file_list:\n","                tmp_label = pd.read_csv(file_path.replace('.csv', '_label.csv'), header=None)\n","            else:\n","                tmp_label = pd.DataFrame(data=([1]*len(tmp_df)), index=None)\n","            df_attack_label = pd.concat([df_attack_label, tmp_label], ignore_index=True)\n","    df_attack['class'] = np.ravel(df_attack_label.values)\n","    df_attack.fillna(0,inplace=True)\n","    return df_attack"],"metadata":{"id":"Vxnz4Y40J3jQ"},"id":"Vxnz4Y40J3jQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Gettings stats from built in isolation forest."],"metadata":{"id":"eieIrLTgSBo3"},"id":"eieIrLTgSBo3"},{"cell_type":"markdown","source":["#### Visualise"],"metadata":{"id":"ncWsY4PMSneo"},"id":"ncWsY4PMSneo"},{"cell_type":"code","source":["from sklearn.tree import _tree\n","\n","# Getting path lengths of each data point.\n","def get_path_length(estimator, X):\n","    n_samples = X.shape[0]\n","    path_lengths = np.zeros(n_samples)\n","    X = X.values\n","    for i in range(n_samples):\n","        node_indicator = estimator.decision_path(X[i].reshape(1, -1)).indices\n","        path_lengths[i] = len(node_indicator) - 1\n","    return path_lengths\n","\n","# Calculating average path length.\n","def average_path_length(iforest, X):\n","    all_path_lengths = np.zeros(X.shape[0])\n","    for estimator in iforest.estimators_:\n","        all_path_lengths += get_path_length(estimator, X)\n","    return all_path_lengths / len(iforest.estimators_)\n","\n","# Plotting the distribution of path lengths for attack data.\n","def justification_plot(a):\n","    clf_model = IsolationForest(n_estimators=50, max_samples=200, random_state=114514,\n","                                contamination=0.1,n_jobs=8)\n","    tmp = pd.concat([x_train,attack_x_train])\n","    tmp.fillna(0,inplace=True)\n","    clf_model.fit(tmp.iloc[:, 1:])\n","    tmp_normal = df_normal_eval.drop(columns=['class'])\n","    tmp_normal = tmp_normal.iloc[:, 1:]\n","    tmp_attack = df_attack_eval.drop(columns=['class'])\n","    tmp_attack = tmp_attack.iloc[:, 1:]\n","\n","    path_lengths_normal = average_path_length(clf_model, tmp_normal)\n","    path_lengths_anomalies = average_path_length(clf_model, tmp_attack)\n","\n","    plt.figure(figsize=(10, 8), dpi = 1000)\n","    plt.scatter(path_lengths_normal, range(len(path_lengths_normal)), color='blue', label='Benign Samples', marker='o')\n","    plt.scatter(path_lengths_anomalies, range(len(path_lengths_anomalies)), color='red', label='Malicious Samples', marker='o')\n","    a = a.replace('_', ' ')\n","    plt.xlabel('Expected Path Lengths', fontsize=30)\n","    plt.ylabel('Samples Distribution', fontsize=30)\n","    plt.xticks(fontsize=30)\n","    plt.yticks([])\n","    plt.legend(loc='upper left',fontsize=30, markerscale=5)\n","    filename = a + \".pdf\"\n","    plt.savefig(filename, dpi=1000, bbox_inches='tight')\n","    plt.show()"],"metadata":{"id":"ekte1eq2MhVf"},"id":"ekte1eq2MhVf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["device_list=['360_camera']\n","attack_list=['http_ddos','data_theft','keylogging','service_scan','tcp_ddos','mirai','os_scan','aidra','bashlite','mirai_router_filter','os_scan_router','port_scan_router','tcp_ddos_router','udp_ddos','udp_ddos_router']\n","\n","df_normal_train_data = load_iot_data_seq(device_list=device_list, begin=0, end=1)\n","contamination = 0.4\n","for a in attack_list:\n","\n","    df_attack_train = load_iot_attack_seq(a)\n","    if contamination != -1:\n","      num = int((1000 / contamination - 1000) / 0.2)\n","      df_normal_train_data = df_normal_train_data.sample(n=num, replace=False, random_state=20)\n","      df_attack_train = df_attack_train.sample(n=5000, replace=False, random_state=20)\n","    df_normal_train, df_normal_eval = train_test_split(df_normal_train_data, test_size=0.2, random_state=20)\n","\n","    df_attack_train, df_attack_eval = train_test_split(df_attack_train, test_size=0.2, random_state=20)\n","    x_train, y_train = df_normal_train.drop(columns=['class']), df_normal_train['class']\n","    attack_x_train, attack_y_train = df_attack_train.drop(columns=['class']), df_attack_train['class']\n","    df_eval = pd.concat([df_normal_eval, df_attack_eval])\n","    x_eval, y_eval = df_eval.drop(columns=['class']), df_eval['class']\n","    eval = x_eval\n","    print(\"------------------------------Attack : \",a, \"--------------------------------\\n\")\n","    justification_plot(a)\n"],"metadata":{"id":"N4azsdHgMY8H"},"id":"N4azsdHgMY8H","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"cf6905b6-95b0-4570-ae00-3a10631f4c17","metadata":{"id":"cf6905b6-95b0-4570-ae00-3a10631f4c17"},"source":["# Train iGuard using Magnifier on 105 features."]},{"cell_type":"code","source":["class Node:\n","    def __init__(self, data, left=None, right=None, depth=0):\n","        self.data = data\n","        self.left = left\n","        self.right = right\n","        self.depth = depth\n","        self.label = None\n","        self.val = None\n","        self.col = None\n","\n","class isolationTree:\n","    def __init__(self, data, depth=0, max_depth=0, thres = None):\n","        self.max_depth = max_depth\n","        self.thres = thres\n","        self.root = self.build_tree(data, depth)\n","        self.model = None\n","        self.num_rows = 50\n","\n","    def build_tree(self, data, depth):\n","        if len(data) == 0:\n","            return None\n","        elif len(data) == 1:\n","            return Node(data, depth=depth)\n","        else:\n","            q = Queue()\n","            root = Node(data, depth=depth)\n","            q.put(root)\n","\n","            while not q.empty():\n","                current_node = q.get()\n","                if len(current_node.data) > 1:\n","                  split_column = random.randint(0, len(current_node.data.iloc[0]) - 1)\n","                  while True:\n","                    if min(current_node.data.iloc[:, split_column]) != max(current_node.data.iloc[:, split_column]):\n","                      break\n","                    split_column = (split_column + 1) % len(current_node.data.iloc[0])\n","                  split_value = random.uniform(min(current_node.data.iloc[:, split_column]), max(current_node.data.iloc[:, split_column]))\n","                  left_data = current_node.data[current_node.data.iloc[:, split_column] <= split_value]\n","                  right_data = current_node.data[current_node.data.iloc[:, split_column] > split_value]\n","                  current_node.val = split_value\n","                  current_node.col = split_column\n","\n","                  if len(left_data) > 0:\n","                      left_node = Node(left_data, depth=current_node.depth + 1)\n","                      current_node.left = left_node\n","                      if len(left_data) != 1:\n","                        q.put(left_node)\n","\n","                  if len(right_data) > 0:\n","                      right_node = Node(right_data, depth=current_node.depth + 1)\n","                      current_node.right = right_node\n","                      if len(right_data) != 1:\n","                        q.put(right_node)\n","            return root\n","\n","    def label_tree(self, root):\n","\n","        if root.left is None and root.right is None:\n","            sampled_df = root.data.sample(n=self.num_rows, replace=True, random_state=42)\n","\n","            noise = np.random.normal(0, 0.01, sampled_df.shape)\n","\n","            sampled_df = scaler.transform(root.data)\n","            sampled_df = sampled_df + noise\n","            sampled_df = np.pad(sampled_df, ((0,0),(3,0)), 'constant')\n","\n","            index = np.empty((0,0))\n","            Port_index = np.arange(4,-1,-1).reshape(5,-1)\n","            MIstat_index = np.arange(5,20).reshape(5,-1)\n","            HHstat_index = np.arange(20,55).reshape(5,-1)\n","            HHstat_jit_index = np.arange(55,70).reshape(5,-1)\n","            HpHpstat_index = np.arange(70,105).reshape(5,-1)\n","            for i in range(5):\n","                index = np.append(index, Port_index[i])\n","                index = np.append(index, MIstat_index[i])\n","                index = np.append(index, HHstat_index[i])\n","                index = np.append(index, HHstat_jit_index[i])\n","                index = np.append(index, HpHpstat_index[i])\n","            sampled_df = sampled_df[:, index.astype(int).tolist()].reshape(-1, 5, 21)\n","            sampled_df = torch.tensor(sampled_df, dtype=torch.float32)\n","            pred, _ = self.model(sampled_df)\n","            rmse = criterion(pred, sampled_df)\n","            rmse = torch.sqrt(rmse)\n","            root.label = 1 if rmse > self.thres else 0\n","        else:\n","            if root.left:\n","              self.label_tree(root.left)\n","            if root.right:\n","              self.label_tree(root.right)\n","\n","    def pred_tree(self, data, node):\n","      if node is None:\n","        return 1, 0\n","\n","      if node.left is None and node.right is None:\n","        return node.label, 0\n","      if data[node.col] <= node.val:\n","        label, path_len = self.pred_tree(data,node.left)\n","        return label, 1 + path_len\n","      if data[node.col] > node.val:\n","        label, path_len = self.pred_tree(data,node.right)\n","        return label, 1 + path_len\n","\n","class isolationForest(nn.Module):\n","    def __init__(self, data, n_trees=100, max_depth=5, subspace=256, model=None):\n","        super().__init__()\n","        self.data = data\n","        self.n_trees = n_trees\n","        self.max_depth = max_depth\n","        self.subspace = subspace\n","        self.model = model\n","        self.thres = 0.4\n","        self.avg_path_len = 2 * (np.log(subspace - 1) + 0.5772) - 2 * (subspace - 1) / subspace\n","        self.trees = []\n","    def forward(self, x):\n","      return None\n","    def fit(self):\n","\n","        for i in range(self.n_trees):\n","            if self.subspace > 1:\n","                subdata = self.data.sample(self.subspace)\n","            else:\n","                subdata = self.data.sample(frac=self.subspace)\n","            tree = isolationTree(subdata, depth=0, max_depth = self.max_depth, thres = self.thres)\n","            self.trees.append(tree)\n","\n","    def label(self, model):\n","        for tree in self.trees:\n","            tree.model = model\n","        for tree in self.trees:\n","            tree.label_tree(tree.root)\n","\n","    def pred(self, data, alpha):\n","      eval = []\n","      for i,row in data.iterrows():\n","          temp = [tree.pred_tree(row, tree.root) for tree in self.trees]\n","          res = min(temp)[1], max(temp)[1]\n","          print(res)\n","          agg_label = sum(x[0] for x in temp) / self.n_trees\n","          avg_len = sum(x[1] for x in temp) / self.n_trees\n","\n","          iso_score = 2 ** (- avg_len / self.avg_path_len)\n","          print(f\"len: {avg_len} score: {iso_score}\")\n","          eval.append(-1 if alpha * agg_label + (1 - alpha) * iso_score > 0.5 else 1)\n","      return eval\n"],"metadata":{"id":"6OlOeCNMPF0X"},"id":"6OlOeCNMPF0X","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def iGuard_control_plane(auto):\n","    tmp = pd.concat([x_train,attack_x_train])\n","    tmp.fillna(0,inplace=True)\n","    tmp = tmp.iloc[:, 1:]\n","    tmp.columns = range(len(tmp.columns))\n","    clf_model = isolationForest(tmp, n_trees=50, max_depth=8, subspace=200)\n","    clf_model.fit()\n","    print(\"Isolation Forest training completed\\n\")\n","    auto = auto.cpu()\n","    clf_model.label(auto)\n","    print(\"Isolation Forest labeling completed\\n\")\n","    x_eval = eval\n","    x_eval.fillna(0,inplace=True)\n","    x_eval = x_eval.iloc[:, 1:]\n","    x_eval.columns = range(len(x_eval.columns))\n","    y_pred_eval = pred(clf_model, x_eval, 1)\n","    eval_y = y_eval\n","    eval_x = x_eval\n","\n","    y_pred_eval = np.array(y_pred_eval)\n","    y_pred_eval[y_pred_eval == 1] = 0\n","    y_pred_eval[y_pred_eval == -1] = 1\n","    temp_str = classification_report(y_true=eval_y, y_pred=y_pred_eval)\n","    temp_list = temp_str.split()\n","    print(\"Our Isolation Forest + Autoencoder\")\n","    print(temp_str)\n","    print(\"Confusion Matrix\")\n","    print(confusion_matrix(eval_y, y_pred_eval))\n","    print(\"\\n F1 Score\")\n","    print(f1_score(eval_y, y_pred_eval))\n","    print(\"\\n roc-auc\")\n","    print(roc_auc_score(eval_y, y_pred_eval))\n","    print(\"\\n pr-auc\")\n","    print(average_precision_score(eval_y, y_pred_eval))\n","    return clf_model"],"metadata":{"id":"RVlh-t2VPmDV"},"id":"RVlh-t2VPmDV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 256\n","TWO_D = True\n","INPUTSIZE = 105\n","TEST_BATCH_SIZE = 60000\n","\n","def train_data_processing(df_normal_train, df_attack_eval, x_eval, y_eval, df_normal_eval, TWO_D):\n","    X_train, y_train= df_normal_train.drop(columns=[0, 'class']), df_normal_train['class']\n","    X_normal_eval = df_normal_eval.drop(columns=[0, 'class'])\n","    X_attack_eval = df_attack_eval.drop(columns=[0, 'class'])\n","    X_train, y_train = X_train.values, y_train.values\n","    X_normal_eval = X_normal_eval.values\n","    X_attack_eval = X_attack_eval.values\n","    X_train = scaler.fit_transform(X_train)\n","    X_normal_eval = scaler.transform(X_normal_eval)\n","    X_attack_eval = scaler.transform(X_attack_eval)\n","    x_eval = x_eval.drop(columns=[0])\n","    x_eval = scaler.transform(x_eval)\n","    y_eval = y_eval.values\n","\n","    if TWO_D:\n","        # Padding\n","        X_train = np.pad(X_train, ((0,0),(3,0)), 'constant')\n","        x_eval = np.pad(x_eval, ((0,0),(3,0)), 'constant')\n","        X_normal_eval = np.pad(X_normal_eval, ((0,0),(3,0)), 'constant')\n","        X_attack_eval = np.pad(X_attack_eval, ((0,0),(3,0)), 'constant')\n","        index = np.empty((0,0))\n","        Port_index = np.arange(4,-1,-1).reshape(5,-1)\n","        MIstat_index = np.arange(5,20).reshape(5,-1)\n","        HHstat_index = np.arange(20,55).reshape(5,-1)\n","        HHstat_jit_index = np.arange(55,70).reshape(5,-1)\n","        HpHpstat_index = np.arange(70,105).reshape(5,-1)\n","        for i in range(5):\n","            index = np.append(index, Port_index[i])\n","            index = np.append(index, MIstat_index[i])\n","            index = np.append(index, HHstat_index[i])\n","            index = np.append(index, HHstat_jit_index[i])\n","            index = np.append(index, HpHpstat_index[i])\n","        X_train = X_train[:, index.astype(int).tolist()].reshape(-1, 5, 21)\n","        x_eval = x_eval[:, index.astype(int).tolist()].reshape(-1, 5, 21)\n","        X_normal_eval = X_normal_eval[:, index.astype(int).tolist()].reshape(-1, 5, 21)\n","        X_attack_eval = X_attack_eval[:, index.astype(int).tolist()].reshape(-1, 5, 21)\n","    X_train = torch.tensor(X_train, dtype=torch.float32)\n","    x_eval = torch.tensor(x_eval, dtype=torch.float32)\n","    X_normal_eval = torch.tensor(X_normal_eval, dtype=torch.float32)\n","    X_attack_eval = torch.tensor(X_attack_eval, dtype=torch.float32)\n","    y_train = torch.tensor(y_train)\n","    y_eval = torch.tensor(y_eval)\n","\n","    return X_train, y_train, x_eval, y_eval, X_normal_eval, X_attack_eval\n","\n","\n","def train_magnifier(model, df_normal_train, df_attack_eval, model_save_path, x_eval, y_eval,  df_normal_eval, TWO_D):\n","    lr = 1e-2\n","    weight_decay = 0.01\n","    epoches = 5\n","    INPUTSIZE = 100\n","\n","    X_train, y_train, x_eval, y_eval, X_normal_eval, X_attack_eval = train_data_processing(df_normal_train, df_attack_eval, x_eval, y_eval, df_normal_eval, TWO_D)\n","    train_datasets = Data.TensorDataset(X_train, y_train)\n","    train_loader = Data.DataLoader(dataset=train_datasets, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n","    test_datasets = Data.TensorDataset(x_eval, y_eval)\n","    test_loader = Data.DataLoader(dataset=test_datasets, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n","\n","    print(\"Total number of Epoch: \", epoches)\n","\n","    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    if torch.cuda.is_available():\n","        model.cuda()\n","\n","    for epoch in range(epoches):\n","        if epoch in [epoches * 0.5, epoches * 1.0]:\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] *= 0.1\n","        model.train()\n","        total_loss = 0\n","        for step, (b_x, b_y) in enumerate(train_loader):\n","            # forward\n","            if torch.cuda.is_available():\n","              b_x = b_x.cuda()\n","            output, encoded = model(b_x)\n","            loss = criterion(b_x, output)\n","            loss = torch.sqrt(loss)\n","            total_loss += loss.sum()\n","            # backward\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        with torch.no_grad():\n","            model.eval()\n","            loss_normal = 0\n","            for i in range(0, X_normal_eval.shape[0], 16):\n","                if torch.cuda.is_available():\n","                  b_x = X_normal_eval[i:i + 16].cuda()\n","                else:\n","                  b_x = X_normal_eval[i:i + 16]\n","                temp_x, _ = model(b_x)\n","                loss_temp = criterion(temp_x, b_x)\n","                loss_normal += loss_temp\n","            loss_normal = (loss_normal.detach().cpu().numpy()) / i\n","            print('the eval normal loss ', loss_normal)\n","            loss_attack = 0\n","            for i in range(0, X_attack_eval.shape[0], 16):\n","                if torch.cuda.is_available():\n","                  b_x = X_attack_eval[i:i + 16].cuda()\n","                temp_x, _ = model(b_x)\n","                loss_temp = criterion(temp_x, b_x)\n","                loss_attack += loss_temp\n","            loss_attack = (loss_attack.detach().cpu().numpy()) / i\n","            print('the eval attack loss ', loss_attack)\n","\n","        print(\"epoch=\", epoch, total_loss)\n","        print(len(X_train))\n","    test_magnifier(model, test_loader, y_eval)\n","    return model\n","\n","def test_magnifier(test_model, test_loader, y_eval):\n","    begin = time.time()\n","    test_model.thr = 0.4\n","\n","    with torch.no_grad():\n","        test_model.eval()\n","        correct = 0.\n","        df_score = []\n","        rmse_list = []\n","        y_pred_eval = []\n","        eval_y = []\n","        for batch_idx, (data, target) in enumerate(test_loader):\n","            if torch.cuda.is_available():\n","              data = data.cuda()\n","            eval_output, _ = test_model(data)\n","            y_pred_eval.extend(test_model.pred(eval_output, data))\n","            eval_y.extend(target)\n","\n","    temp_str = classification_report(y_true=eval_y, y_pred=y_pred_eval)\n","    temp_list = temp_str.split()\n","    print(\"Autoencoder\")\n","    print(temp_str)\n","    print(\"Confusion Matrix\")\n","    print(confusion_matrix(eval_y, y_pred_eval))\n","    print(\"\\n F1 Score\")\n","    print(f1_score(eval_y, y_pred_eval))\n","    print(\"\\n roc-auc\")\n","    print(roc_auc_score(eval_y, y_pred_eval))\n","    print(\"\\n pr-auc\")\n","    print(average_precision_score(eval_y, y_pred_eval))\n","\n","def magnifier_trainer():\n","    torch.manual_seed(42)\n","    torch.cuda.manual_seed(42)\n","    model = Magnifier(input_size=INPUTSIZE, seed=42)\n","    magnifier = train_magnifier(model, df_normal_train, df_attack_eval, model_save_path, x_eval, y_eval, df_normal_eval, TWO_D)\n","    print(\"Autoencoder training completed\\n\")\n","    return magnifier"],"metadata":{"id":"4Vm0qFK2QNNj"},"id":"4Vm0qFK2QNNj","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sk_iforest():\n","    sklearn_clf_model = IsolationForest(n_estimators=50, max_samples=200, random_state=114514,\n","                             contamination=0.2,n_jobs=8)\n","\n","    tmp = pd.concat([x_train,attack_x_train])\n","    tmp.fillna(0,inplace=True)\n","    sklearn_clf_model.fit(tmp.iloc[:, 1:])\n","\n","    y_pred_eval = sklearn_clf_model.predict(x_eval.iloc[:, 1:])\n","    eval_y = y_eval\n","    eval_x = x_eval\n","\n","    y_pred_eval[y_pred_eval == 1] = 0\n","    y_pred_eval[y_pred_eval == -1] = 1\n","    temp_str = classification_report(y_true=eval_y, y_pred=y_pred_eval)\n","    temp_list = temp_str.split()\n","    print(\"sklearn Isolation Forest\")\n","    print(temp_str)\n","    print(\"Confusion Matrix\")\n","    print(confusion_matrix(eval_y, y_pred_eval))\n","    print(\"\\n F1 Score\")\n","    print(f1_score(eval_y, y_pred_eval))\n","    print(\"\\n roc-auc\")\n","    print(roc_auc_score(eval_y, y_pred_eval))\n","    print(\"\\n pr-auc\")\n","    print(average_precision_score(eval_y, y_pred_eval))"],"metadata":{"id":"Imx2Eh6QWVLC"},"id":"Imx2Eh6QWVLC","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c0816221-5f69-478d-982c-24445194a4f9","metadata":{"id":"c0816221-5f69-478d-982c-24445194a4f9"},"source":["## Experiment 2: Fig. 5 and Appendix Fig. 8"]},{"cell_type":"code","execution_count":null,"id":"96db1772-5994-40ce-86d9-bf7e9171ea71","metadata":{"id":"96db1772-5994-40ce-86d9-bf7e9171ea71"},"outputs":[],"source":["# Use iGuard_control_plane to obtain comparison with Magnifier and iForest on F1 score, ROC_AUC and ROC_PR on best configuration.\n","# Refer to your google doc\n","\n","device_list=['360_camera']\n","attack_list=['http_ddos','data_theft','keylogging','service_scan','tcp_ddos','mirai','os_scan','aidra','bashlite','mirai_router_filter','os_scan_router','port_scan_router','tcp_ddos_router','udp_ddos','udp_ddos_router']\n","\n","df_normal_train_data = load_iot_data_seq(device_list=device_list, begin=0, end=1)\n","contamination = 0.4\n","for a in attack_list:\n","\n","    df_attack_train = load_iot_attack_seq(a)\n","    if contamination != -1:\n","      num = int((1000 / contamination - 1000) / 0.2)\n","      df_normal_train_data = df_normal_train_data.sample(n=num, replace=False, random_state=20)\n","      df_attack_train = df_attack_train.sample(n=5000, replace=False, random_state=20)\n","    df_normal_train, df_normal_eval = train_test_split(df_normal_train_data, test_size=0.2, random_state=20)\n","\n","    df_attack_train, df_attack_eval = train_test_split(df_attack_train, test_size=0.2, random_state=20)\n","    x_train, y_train = df_normal_train.drop(columns=['class']), df_normal_train['class']\n","    attack_x_train, attack_y_train = df_attack_train.drop(columns=['class']), df_attack_train['class']\n","    df_eval = pd.concat([df_normal_eval, df_attack_eval])\n","    x_eval, y_eval = df_eval.drop(columns=['class']), df_eval['class']\n","    eval = x_eval\n","    print(\"------------------------------Attack : \",a, \"--------------------------------\\n\")\n","    sk_iforest()\n","    magnifier = magnifier_trainer()\n","    iGuard = iGuard_control_plane(magnifier)"]},{"cell_type":"markdown","id":"108c5709-dc53-4115-8c05-d54d63f5cf7b","metadata":{"id":"108c5709-dc53-4115-8c05-d54d63f5cf7b"},"source":["# Train iGuard on data plane features using custom autoencoder."]},{"cell_type":"code","source":["# def sk_iforest(a):\n","#     sklearn_clf_model = IsolationForest(n_estimators=50, max_samples=200, random_state=114514,\n","#                              contamination=0.2,n_jobs=8)\n","#     tmp = pd.concat([x_train,attack_x_train])\n","#     tmp.fillna(0,inplace=True)\n","#     sklearn_clf_model.fit(tmp)\n","#     y_pred_eval = sklearn_clf_model.predict(x_eval)\n","#     eval_y = y_eval\n","#     eval_x = x_eval\n","\n","#     y_pred_eval[y_pred_eval == 1] = 0\n","#     y_pred_eval[y_pred_eval == -1] = 1\n","#     temp_str = classification_report(y_true=eval_y, y_pred=y_pred_eval)\n","#     temp_list = temp_str.split()\n","#     print(\"sklearn Isolation Forest\")\n","#     print(temp_str)\n","#     print(\"Confusion Matrix\")\n","#     print(confusion_matrix(eval_y, y_pred_eval))\n","#     print(\"\\n F1 Score\")\n","#     print(f1_score(eval_y, y_pred_eval))\n","#     print(\"\\n roc-auc\")\n","#     print(roc_auc_score(eval_y, y_pred_eval))\n","#     print(\"\\n pr-auc\")\n","#     print(average_precision_score(eval_y, y_pred_eval))\n","#     name = 'iForest'+a+'.pkl'\n","#     with open(name, 'wb') as f:\n","#         pickle.dump(sklearn_clf_model, f)"],"metadata":{"id":"Iz7wKvyKciDz"},"id":"Iz7wKvyKciDz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DilatedSeparableConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, dilation):\n","        super(DilatedSeparableConv, self).__init__()\n","        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=dilation, dilation=dilation, groups=in_channels)\n","        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        x = self.pointwise(x)\n","        return x\n","\n","class Autoencoder(nn.Module):\n","    def __init__(self, input_dim, latent_dim):\n","        super(Autoencoder, self).__init__()\n","        torch.manual_seed(42)\n","        torch.cuda.manual_seed(42)\n","        self.encoder = nn.Sequential(\n","            DilatedSeparableConv(1, 16, dilation=1),\n","            nn.ReLU(True),\n","            nn.MaxPool1d(2),\n","            DilatedSeparableConv(16, 32, dilation=2),\n","            nn.ReLU(True),\n","            nn.MaxPool1d(2),\n","            DilatedSeparableConv(32, 64, dilation=4),\n","            nn.ReLU(True),\n","            nn.MaxPool1d(2),\n","            nn.Conv1d(64, latent_dim, kernel_size=1)\n","        )\n","\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose1d(latent_dim, 64, kernel_size=1),\n","            nn.ReLU(True),\n","            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(True),\n","            nn.ConvTranspose1d(32, 16, kernel_size=5, stride=2, padding=1),\n","            nn.ReLU(True),\n","            nn.ConvTranspose1d(16, 1, kernel_size=6, stride=2, padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"],"metadata":{"id":"jpjAEhHobWjv"},"id":"jpjAEhHobWjv","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_custom_autoencoder(df_normal_train, df_normal_eval, df_attack_eval):\n","    X_train = df_normal_train.values\n","    X_test = pd.concat([df_normal_eval, df_attack_eval]).values\n","    actual = torch.cat([torch.zeros(df_normal_eval.shape[0]), torch.ones(df_attack_eval.shape[0])])\n","    X_train = scaler.fit_transform(X_train)\n","    X_test = scaler.transform(X_test)\n","\n","    input_dim = X_train.shape[1]\n","    encoding_dim = 8\n","    num_epochs = 1000\n","    batch_size = 512\n","\n","    autoencoder = Autoencoder(input_dim, encoding_dim)\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n","    #optimizer = optim.Adagrad(autoencoder.parameters(), lr=0.01)\n","    #optimizer = optim.RMSprop(autoencoder.parameters(), lr=0.001, alpha=0.9)\n","    #optimizer = optim.SGD(autoencoder.parameters(), lr=0.1, weight_decay=0.001)\n","\n","    autoencoder = autoencoder.cuda()\n","\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        for i in range(0, len(X_train), batch_size):\n","            inputs = torch.tensor(X_train[i:i+batch_size], dtype=torch.float).unsqueeze(0)\n","            inputs = inputs.cuda()\n","            inputs = inputs.transpose(0, 1)\n","            outputs = autoencoder(inputs)\n","            loss = criterion(outputs, inputs)\n","            total_loss += loss\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        if (epoch+1) % 200 == 0:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.sum():.4f}')\n","\n","    autoencoder.eval()\n","    with torch.no_grad():\n","        input = torch.tensor(X_test, dtype=torch.float).unsqueeze(0).transpose(0, 1).cuda()\n","        decoded_data = autoencoder(input).cpu().numpy()\n","    decoded_data_binary = decoded_data.squeeze()\n","    mse = np.mean(np.power(X_test - decoded_data_binary, 2), axis=1)\n","    rmse = np.sqrt(mse)\n","\n","    for i in [0.001]:\n","        thres = i\n","        print(\"-------------i\",i)\n","        predicted = pd.Series(np.where(rmse > thres, 1, 0),dtype=\"float64\")\n","        f1 = f1_score(actual, predicted)\n","        print(confusion_matrix(actual, predicted))\n","        print(\"\\n Classification report\")\n","        print(classification_report(actual, predicted))\n","        print('F1 Score: ', f1)\n","\n","    return autoencoder"],"metadata":{"id":"Z3FVnUn7b3_A"},"id":"Z3FVnUn7b3_A","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def custom_autoencoder():\n","    torch.manual_seed(42)\n","    torch.cuda.manual_seed(42)\n","    auto = train_custom_autoencoder(df_norm_train.iloc[:,:12], df_normal_eval.iloc[:,:12], df_attack_eval.iloc[:,:12])\n","    print(\"Autoencoder training completed\\n\")\n","    return auto"],"metadata":{"id":"iE5mpBo1cc7T"},"id":"iE5mpBo1cc7T","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def iGuard_data_plane(auto,a):\n","    tmp = pd.concat([x_train,attack_x_train])\n","    tmp.fillna(0,inplace=True)\n","    tmp.columns = range(len(tmp.columns))\n","    clf_model = isolationForest(tmp, n_trees=50, max_depth=8, subspace=200)\n","    clf_model.fit()\n","    print(\"Isolation Forest training completed\\n\")\n","    auto = auto.cpu()\n","    clf_model.label(auto)\n","    print(\"Isolation Forest labeling completed\\n\")\n","    x_eval = eval\n","    x_eval.fillna(0,inplace=True)\n","    x_eval.columns = range(len(x_eval.columns))\n","    y_pred_eval = pred(clf_model, x_eval, 1)\n","    eval_y = y_eval\n","    eval_x = x_eval\n","\n","    y_pred_eval = np.array(y_pred_eval)\n","    y_pred_eval[y_pred_eval == 1] = 0\n","    y_pred_eval[y_pred_eval == -1] = 1\n","    print(\"Our Isolation Forest + Autoencoder\")\n","    print(\"Confusion Matrix\")\n","    print(confusion_matrix(eval_y, y_pred_eval))\n","    print(\"\\n Classification report\")\n","    print(classification_report(eval_y, y_pred_eval))\n","    print(\"\\n F1 Score\")\n","    print(f1_score(eval_y, y_pred_eval))\n","    print(\"\\n roc-auc\")\n","    print(roc_auc_score(eval_y, y_pred_eval))\n","    print(\"\\n pr-auc\")\n","    print(average_precision_score(eval_y, y_pred_eval))\n","    name = 'iGuard'+a+'.pkl'\n","    with open(name, 'wb') as f:\n","        pickle.dump(clf_model, f)"],"metadata":{"id":"9YmFTrWucUH7"},"id":"9YmFTrWucUH7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["device_list=['360_camera']\n","attack_list=['http_ddos','data_theft','keylogging','service_scan','tcp_ddos','mirai','os_scan','aidra','bashlite','mirai_router_filter','os_scan_router','port_scan_router','tcp_ddos_router','udp_ddos','udp_ddos_router']\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","contamination = 0.2\n","for a in attack_list:\n","\n","    normal_path = \"/content/drive/MyDrive/HorusEye/DataSets/Dataplane/Normal/\" + device_list[0] + \".csv\"\n","    attack_path = \"/content/drive/MyDrive/HorusEye/DataSets/Dataplane/Anomaly/\" + a + \".csv\"\n","    df_normal_train_data = pd.read_csv(normal_path)\n","    df_attack_train = pd.read_csv(attack_path)\n","    df_normal_train_data = df_normal_train_data.drop(columns=['tcp_udp','dst_port','n_packets'])\n","    df_attack_train = df_attack_train.drop(columns=['tcp_udp','dst_port','n_packets'])\n","    df_normal_train_data = df_normal_train_data.applymap(int)\n","    df_attack_train = df_attack_train.applymap(int)\n","    datafetch = True\n","    n1 = 200\n","    n2 = 1000\n","    while(datafetch):\n","        if contamination != -1:\n","            try:\n","                num = int((n1 / contamination - n1) / 0.2)\n","                temp = df_normal_train_data.sample(n=num, replace=False, random_state=20)\n","                df_norm_train = df_normal_train_data.drop(temp.index)\n","\n","                df_attack_train = df_attack_train.sample(n=n2, replace=False, random_state=20)\n","                df_normal_train_data = temp\n","                datafetch = False\n","            except ValueError as e:\n","                if \"Cannot take a larger sample than population when 'replace=False'\" in str(e):\n","                    n1 = int(n1/2)\n","                    n2 = int(n2/2)\n","                else:\n","                    raise e\n","\n","    df_normal_train, df_normal_eval = train_test_split(df_normal_train_data, test_size=0.2, random_state=20)\n","    df_norm_train = pd.concat([df_norm_train, df_normal_train])\n","    df_attack_train, df_attack_eval = train_test_split(df_attack_train, test_size=0.2, random_state=20)\n","    x_train, y_train = df_normal_train.drop(columns=['class']), df_normal_train['class']\n","    attack_x_train, attack_y_train = df_attack_train.drop(columns=['class']), df_attack_train['class']\n","    df_eval = pd.concat([df_normal_eval, df_attack_eval])\n","    x_eval, y_eval = df_eval.drop(columns=['class']), df_eval['class']\n","    eval = x_eval\n","    print(\"------------------------------Attack : \",a, \"--------------------------------\\n\")\n","    sk_iforest(a)\n","    auto = custom_autoencoder()\n","    iGuard_data_plane(auto,a)"],"metadata":{"id":"E2uD_K2kcO2h"},"id":"E2uD_K2kcO2h","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"946c3b79-c436-4399-a5d1-72960bc4b49f","metadata":{"id":"946c3b79-c436-4399-a5d1-72960bc4b49f"},"source":["## Experiment 3: Fig. 6, Appendix Fig. 9, and Tables 2, 3 (Data Plane Simulation)"]},{"cell_type":"code","execution_count":null,"id":"797d08f5-2c71-41d8-b053-67a3c9701dd9","metadata":{"id":"797d08f5-2c71-41d8-b053-67a3c9701dd9"},"outputs":[],"source":["# Use iGuard_data_plane to obtain comparison with Magnifier and iForest on F1 score, ROC_AUC and ROC_PR on best configuration.\n","# Refer to your google doc"]},{"cell_type":"markdown","id":"7d7c48f9-1049-4b8a-af29-a3b4640e681c","metadata":{"id":"7d7c48f9-1049-4b8a-af29-a3b4640e681c"},"source":["## Experiment 4: Comparison with VAE (See Fig. 10 in Appendix). Compare Magnifier with VAE would suffice."]},{"cell_type":"code","execution_count":null,"id":"b81112bc-bc34-4ed0-8ab6-4932adcda009","metadata":{"id":"b81112bc-bc34-4ed0-8ab6-4932adcda009"},"outputs":[],"source":["# Define the VAE model with 4 hidden layers\n","class VAE(nn.Module):\n","    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, latent_dim):\n","        super(VAE, self).__init__()\n","        # Encoder\n","        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n","        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n","        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n","        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n","        self.fc21 = nn.Linear(hidden_dim4, latent_dim)  # Mean\n","        self.fc22 = nn.Linear(hidden_dim4, latent_dim)  # Log variance\n","\n","        # Decoder\n","        self.fc5 = nn.Linear(latent_dim, hidden_dim4)\n","        self.fc6 = nn.Linear(hidden_dim4, hidden_dim3)\n","        self.fc7 = nn.Linear(hidden_dim3, hidden_dim2)\n","        self.fc8 = nn.Linear(hidden_dim2, hidden_dim1)\n","        self.fc9 = nn.Linear(hidden_dim1, input_dim)\n","\n","    def encode(self, x):\n","        h1 = torch.relu(self.fc1(x))\n","        h2 = torch.relu(self.fc2(h1))\n","        h3 = torch.relu(self.fc3(h2))\n","        h4 = torch.relu(self.fc4(h3))\n","        return self.fc21(h4), self.fc22(h4)\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def decode(self, z):\n","        h5 = torch.relu(self.fc5(z))\n","        h6 = torch.relu(self.fc6(h5))\n","        h7 = torch.relu(self.fc7(h6))\n","        h8 = torch.relu(self.fc8(h7))\n","        return torch.sigmoid(self.fc9(h8))\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar\n","\n","    def pred(self, x, threshold):\n","        recon, _, _ = self.forward(x)\n","        mse_loss = F.mse_loss(recon, x, reduction='none')\n","        mse_per_sample = mse_loss.mean(dim=1)\n","        predictions = (mse_per_sample > threshold).float()  # Convert boolean to float (0 or 1)\n","\n","        return predictions\n","\n","def loss_function(recon_x, x, mu, logvar):\n","    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    return BCE + KLD\n","\n","# Hyperparameters\n","input_dim = 103\n","hidden_dim1 = 128\n","hidden_dim2 = 64\n","hidden_dim3 = 32\n","hidden_dim4 = 16\n","latent_dim = 32\n","learning_rate = 0.001\n","num_epochs = 50\n","batch_size = 32\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ca12164d-8555-4d7c-a130-325a7ac6dc74","metadata":{"id":"ca12164d-8555-4d7c-a130-325a7ac6dc74"},"outputs":[],"source":["device_list=['360_camera']\n","attack_list=['http_ddos','data_theft','keylogging','service_scan','tcp_ddos','mirai','os_scan','aidra','bashlite','mirai_router_filter','os_scan_router','port_scan_router','tcp_ddos_router','udp_ddos','udp_ddos_router']\n","\n","for a in attack_list:\n","    df_normal_train_data = load_iot_data_seq(device_list=device_list, begin=0, end=1)\n","    df_attack_train = load_iot_attack_seq(a)\n","\n","    df_normal_train, df_normal_eval = train_test_split(df_normal_train_data, test_size=0.2, random_state=20)\n","    df_attack_train, df_attack_eval = train_test_split(df_attack_train, test_size=0.2, random_state=20)\n","\n","    df_eval = pd.concat([df_normal_eval,df_attack_eval], ignore_index=True)\n","\n","    X_train, y_train = df_normal_train.drop(columns=['class']), df_normal_train['class']\n","    X_attack_test, y_attack_test = df_attack_train.drop(columns=['class']), df_attack_train['class']\n","    X_normal_test, y_noraml_test = df_normal_eval.drop(columns=['class']), df_normal_eval['class']\n","    df_X_test, df_y_test = df_eval.drop(columns=['class']), df_eval['class']\n","\n","    X_train = scaler.fit_transform(X_train.values)\n","    X_attack_test = scaler.transform(X_attack_test.values)\n","    X_normal_test = scaler.transform(X_normal_test.values)\n","    df_X_test = scaler.transform(df_X_test.values)\n","\n","    X_train = torch.tensor(X_train, dtype=torch.float32)\n","    y_train = torch.tensor(y_train.values, dtype=torch.float32)\n","\n","    X_attack_test = torch.tensor(X_attack_test, dtype=torch.float32)\n","    y_attack_test = torch.tensor(y_attack_test.values, dtype=torch.float32)\n","    X_normal_test = torch.tensor(X_normal_test, dtype=torch.float32)\n","    y_noraml_test = torch.tensor(y_noraml_test.values, dtype=torch.float32)\n","    df_X_test = torch.tensor(df_X_test, dtype=torch.float32)\n","    df_y_test = torch.tensor(df_y_test.values, dtype=torch.float32)\n","\n","    train_datasets = TensorDataset(X_train)\n","    train_loader = DataLoader(dataset=train_datasets, batch_size=256, shuffle=True, num_workers=0)\n","    normal_test_datasets = TensorDataset(X_normal_test, y_noraml_test)\n","    normal_test_loader = DataLoader(dataset=normal_test_datasets, batch_size=256, shuffle=True, num_workers=0)\n","    attack_test_datasets = TensorDataset(X_attack_test, y_attack_test)\n","    attack_test_loader = DataLoader(dataset=attack_test_datasets, batch_size=256, shuffle=True, num_workers=0)\n","    test_datasets = TensorDataset(df_X_test, df_y_test)\n","    test_loader = DataLoader(dataset=test_datasets, batch_size=256, shuffle=True, num_workers=0)\n","    print(\"------------------------------Attack : \",a, \"--------------------------------\\n\")\n","\n","    model = VAE(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, latent_dim)\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    model.train()\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        for batch_idx, (x,) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            recon_batch, mu, logvar = model(x)\n","            loss = loss_function(recon_batch, x, mu, logvar)\n","            total_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","\n","        if epoch % 10 == 0:\n","            print(f'Epoch {epoch}, Loss: {total_loss}')\n","\n","    print(\"Training complete!\")\n","\n","    with torch.no_grad():\n","        model.eval()\n","        loss_normal = 0\n","        for batch_idx, (x, y) in enumerate(normal_test_loader):\n","            b_x = x\n","            temp_x, _, _ = model(b_x)\n","            loss_temp = criterion(temp_x, b_x)\n","            loss_normal += loss_temp\n","        loss_normal = (loss_normal.detach().cpu().numpy())\n","        print('the eval normal loss ', loss_normal)\n","        loss_attack = 0\n","        for batch_idx, (x, y) in enumerate(attack_test_loader):\n","            b_x = x\n","            temp_x, _, _ = model(b_x)\n","            loss_temp = criterion(temp_x, b_x)\n","            loss_attack += loss_temp\n","        loss_attack = (loss_attack.detach().cpu().numpy())\n","\n","        print('the eval attack loss ', loss_attack)\n","\n","    with torch.no_grad():\n","        model.eval()\n","        correct = 0.\n","        df_score = []\n","        rmse_list = []\n","        y_pred_eval = []\n","        eval_y = []\n","        for batch_idx, (data, target) in enumerate(test_loader):\n","            y_pred_eval.extend(model.pred(data, 0.1))\n","            eval_y.extend(target)\n","\n","    temp_str = classification_report(y_true=eval_y, y_pred=y_pred_eval)\n","    temp_list = temp_str.split()\n","    print(\"VAE\")\n","    print(temp_str)\n","    print(\"Confusion Matrix\")\n","    print(confusion_matrix(eval_y, y_pred_eval))\n","    print(\"\\n F1 Score\")\n","    print(f1_score(eval_y, y_pred_eval))\n","    print(\"\\n roc-auc\")\n","    print(roc_auc_score(eval_y, y_pred_eval))\n","    print(\"\\n pr-auc\")\n","    print(average_precision_score(eval_y, y_pred_eval))"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}