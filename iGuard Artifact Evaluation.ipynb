{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31155cfe-d347-4bfd-aa69-dcb6204185c2",
   "metadata": {
    "id": "31155cfe-d347-4bfd-aa69-dcb6204185c2"
   },
   "source": [
    "## Refer to paper, codebase and google doc mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c4bdb-9b62-4bae-8642-878f095cb91b",
   "metadata": {
    "id": "ea8c4bdb-9b62-4bae-8642-878f095cb91b"
   },
   "source": [
    "## Experiment 1: Refer to Fig. 2 and Appendix Fig. 7. Use 105 features of Magnifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leNL3eUBJhRy",
   "metadata": {
    "id": "leNL3eUBJhRy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, classification_report, average_precision_score\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from fcntl import F_SETFL\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from queue import Queue\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "criterion = nn.MSELoss()\n",
    "scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "khuQcVWdJzM5",
   "metadata": {
    "id": "khuQcVWdJzM5"
   },
   "outputs": [],
   "source": [
    "# Returning all files in directory.\n",
    "def file_name_walk(file_dir):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] == \".csv\":\n",
    "                file_list.append(\"{}/{}\".format(root, file))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2lgISN1mJ1Kt",
   "metadata": {
    "id": "2lgISN1mJ1Kt"
   },
   "outputs": [],
   "source": [
    "# Loading benign data from a list of devices.\n",
    "def load_iot_data_seq(device_list=['philips_camera'],begin=0,end=5):\n",
    "    df_normal = pd.DataFrame()\n",
    "    normal_list = device_list\n",
    "    for type_index, type_name in enumerate(normal_list):\n",
    "        file_list = file_name_walk(\n",
    "            './DataSets/normal-kitsune_test/{:}'.format(type_name))\n",
    "        file_list.sort()\n",
    "        df_normal_type = pd.DataFrame()\n",
    "        begin_num = begin\n",
    "        end_num = end\n",
    "        for i, file_path in enumerate(file_list[begin_num:end_num]):\n",
    "            try:\n",
    "                tmp_df = pd.read_csv(file_path, header=None)\n",
    "                df_normal_type = pd.concat([df_normal_type,tmp_df], ignore_index=True)\n",
    "                #df_normal_type = df_normal_type.append(tmp_df, ignore_index=True)\n",
    "            except:\n",
    "                print(file_path)\n",
    "        df_normal = pd.concat([df_normal,df_normal_type], ignore_index=True)\n",
    "        #df_normal = df_normal.append(df_normal_type, ignore_index=True)\n",
    "    df_normal['class'] = 0\n",
    "    df_normal.fillna(0,inplace=True)\n",
    "    return df_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnz4Y40J3jQ",
   "metadata": {
    "id": "Vxnz4Y40J3jQ"
   },
   "outputs": [],
   "source": [
    "# Loading malicious data from a list of attacks.\n",
    "def load_iot_attack_seq(attack_name='all'):\n",
    "    df_attack = pd.DataFrame()\n",
    "    df_attack_label = pd.DataFrame()\n",
    "    #Please adjust the dataset path wherever possible\n",
    "    attack_path = './DataSets/Anomaly/attack_kitsune/'\n",
    "    if (attack_name=='all'):\n",
    "        attack_list=os.listdir(attack_path)\n",
    "    else:\n",
    "        attack_list = [attack_name]\n",
    "    for type_index, type_name in enumerate(attack_list):\n",
    "        file_list = file_name_walk('./DataSets/Anomaly/attack_kitsune/{:}'.format(type_name))\n",
    "        for i, file_path in enumerate(file_list):\n",
    "            if 'label' in file_path:\n",
    "                continue\n",
    "            tmp_df = pd.read_csv(file_path, header=None)\n",
    "            df_attack = pd.concat([df_attack, tmp_df], ignore_index=True)\n",
    "            if file_path.replace('.csv', '_label.csv') in file_list:\n",
    "                tmp_label = pd.read_csv(file_path.replace('.csv', '_label.csv'), header=None)\n",
    "            else:\n",
    "                tmp_label = pd.DataFrame(data=([1]*len(tmp_df)), index=None)\n",
    "            df_attack_label = pd.concat([df_attack_label, tmp_label], ignore_index=True)\n",
    "    df_attack['class'] = np.ravel(df_attack_label.values)\n",
    "    df_attack.fillna(0,inplace=True)\n",
    "    return df_attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eieIrLTgSBo3",
   "metadata": {
    "id": "eieIrLTgSBo3"
   },
   "source": [
    "#### Gettings stats from built in isolation forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ncWsY4PMSneo",
   "metadata": {
    "id": "ncWsY4PMSneo"
   },
   "source": [
    "#### Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ekte1eq2MhVf",
   "metadata": {
    "id": "ekte1eq2MhVf"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import _tree\n",
    "\n",
    "# Getting path lengths of each data point.\n",
    "def get_path_length(estimator, X):\n",
    "    n_samples = X.shape[0]\n",
    "    path_lengths = np.zeros(n_samples)\n",
    "    X = X.values\n",
    "    for i in range(n_samples):\n",
    "        node_indicator = estimator.decision_path(X[i].reshape(1, -1)).indices\n",
    "        path_lengths[i] = len(node_indicator) - 1\n",
    "    return path_lengths\n",
    "\n",
    "# Calculating average path length.\n",
    "def average_path_length(iforest, X):\n",
    "    all_path_lengths = np.zeros(X.shape[0])\n",
    "    for estimator in iforest.estimators_:\n",
    "        all_path_lengths += get_path_length(estimator, X)\n",
    "    return all_path_lengths / len(iforest.estimators_)\n",
    "\n",
    "# Plotting the distribution of path lengths for attack data.\n",
    "def justification_plot(a):\n",
    "    clf_model = IsolationForest(n_estimators=50, max_samples=200, random_state=114514,\n",
    "                                contamination=0.1,n_jobs=8)\n",
    "    tmp = pd.concat([x_train,attack_x_train])\n",
    "    tmp.fillna(0,inplace=True)\n",
    "    clf_model.fit(tmp.iloc[:, 1:])\n",
    "    tmp_normal = df_normal_eval.drop(columns=['class'])\n",
    "    tmp_normal = tmp_normal.iloc[:, 1:]\n",
    "    tmp_attack = df_attack_eval.drop(columns=['class'])\n",
    "    tmp_attack = tmp_attack.iloc[:, 1:]\n",
    "\n",
    "    path_lengths_normal = average_path_length(clf_model, tmp_normal)\n",
    "    path_lengths_anomalies = average_path_length(clf_model, tmp_attack)\n",
    "\n",
    "    plt.figure(figsize=(10, 8), dpi = 1000)\n",
    "    plt.scatter(path_lengths_normal, range(len(path_lengths_normal)), color='blue', label='Benign Samples', marker='o')\n",
    "    plt.scatter(path_lengths_anomalies, range(len(path_lengths_anomalies)), color='red', label='Malicious Samples', marker='o')\n",
    "    a = a.replace('_', ' ')\n",
    "    plt.xlabel('Expected Path Lengths', fontsize=30)\n",
    "    plt.ylabel('Samples Distribution', fontsize=30)\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.yticks([])\n",
    "    plt.legend(loc='upper left',fontsize=30, markerscale=5)\n",
    "    filename = a + \".pdf\"\n",
    "    plt.savefig(filename, dpi=1000, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N4azsdHgMY8H",
   "metadata": {
    "id": "N4azsdHgMY8H"
   },
   "outputs": [],
   "source": [
    "device_list=['360_camera']\n",
    "attack_list=['http_ddos','data_theft','keylogging','service_scan','tcp_ddos','mirai','os_scan','aidra','bashlite','mirai_router_filter','os_scan_router','port_scan_router','tcp_ddos_router','udp_ddos','udp_ddos_router']\n",
    "\n",
    "df_normal_train_data = load_iot_data_seq(device_list=device_list, begin=0, end=1)\n",
    "contamination = 0.4\n",
    "for a in attack_list:\n",
    "\n",
    "    df_attack_train = load_iot_attack_seq(a)\n",
    "    if contamination != -1:\n",
    "      num = int((1000 / contamination - 1000) / 0.2)\n",
    "      df_normal_train_data = df_normal_train_data.sample(n=num, replace=False, random_state=20)\n",
    "      df_attack_train = df_attack_train.sample(n=5000, replace=False, random_state=20)\n",
    "    df_normal_train, df_normal_eval = train_test_split(df_normal_train_data, test_size=0.2, random_state=20)\n",
    "\n",
    "    df_attack_train, df_attack_eval = train_test_split(df_attack_train, test_size=0.2, random_state=20)\n",
    "    x_train, y_train = df_normal_train.drop(columns=['class']), df_normal_train['class']\n",
    "    attack_x_train, attack_y_train = df_attack_train.drop(columns=['class']), df_attack_train['class']\n",
    "    df_eval = pd.concat([df_normal_eval, df_attack_eval])\n",
    "    x_eval, y_eval = df_eval.drop(columns=['class']), df_eval['class']\n",
    "    eval = x_eval\n",
    "    print(\"------------------------------Attack : \",a, \"--------------------------------\\n\")\n",
    "    justification_plot(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09056834-7ecd-403c-bc2e-21dac22b35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class hswish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        out = x * F.relu6(x + 3, inplace=True) / 6\n",
    "        return out\n",
    "\n",
    "\n",
    "class Magnifier(nn.Module):\n",
    "    def __init__(self, input_size=16, seed=42):\n",
    "        super(Magnifier, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        self.input_size = input_size\n",
    "        self.thr = 1\n",
    "        d_dim = 20\n",
    "        self._stage_out_channels = 80\n",
    "        self.flatten_size = int(math.ceil(self.input_size/(3*5))*self._stage_out_channels)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=5, out_channels=d_dim, kernel_size=3,  # 20\n",
    "                      stride=1, dilation=1, padding=1, bias=False, groups=1),\n",
    "            nn.BatchNorm1d(d_dim),\n",
    "            hswish()\n",
    "        )\n",
    "        output_channels = 2*d_dim\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=d_dim, out_channels=d_dim, kernel_size=3,  # 20\n",
    "                      stride=1, dilation=2, padding=2, bias=False, groups=d_dim),\n",
    "            nn.BatchNorm1d(d_dim),\n",
    "            nn.Conv1d(in_channels=d_dim, out_channels=output_channels, kernel_size=1,  # 20\n",
    "                      stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(output_channels),\n",
    "            hswish()\n",
    "        )\n",
    "        input_channels = output_channels\n",
    "        output_channels = self._stage_out_channels\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_channels, out_channels=input_channels, kernel_size=3,  # 20\n",
    "                      stride=1, dilation=3, padding=3, bias=False, groups=input_channels),\n",
    "            nn.BatchNorm1d(input_channels),\n",
    "            nn.Conv1d(in_channels=input_channels, out_channels=output_channels, kernel_size=1,  # 20\n",
    "                      stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(output_channels),\n",
    "            hswish()\n",
    "        )\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=3, stride=3, padding=1)  # upper(20/3)=7\n",
    "        input_channels = output_channels\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_channels, out_channels=output_channels, kernel_size=3,\n",
    "                      stride=1, dilation=1, padding=1, bias=False, groups=1),  # 7\n",
    "            nn.BatchNorm1d(output_channels),\n",
    "            hswish(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.flatten_size, self.input_size), # 7*80=560\n",
    "            nn.BatchNorm1d(1),\n",
    "            hswish()\n",
    "        )\n",
    "        self.flatten = nn.Sequential(\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.flatten(out)\n",
    "        hidden = torch.unsqueeze(out, dim=1)\n",
    "        output = self.decoder(hidden)\n",
    "        output = torch.reshape(output, (-1,5,int(self.input_size/5)))\n",
    "        return output, hidden\n",
    "    \n",
    "    def excute_RMSE(self, input, target):\n",
    "        input = torch.reshape(input, (-1,1,self.input_size))\n",
    "        target = torch.reshape(target, (-1,1,self.input_size))\n",
    "        mse = (input - target).pow(2).sum(2) / self.input_size\n",
    "        rmse = torch.sqrt(mse)\n",
    "        return rmse.detach().cpu().numpy().reshape(-1)  # 0 :error 1:normal\n",
    "\n",
    "    def pred(self, input, target):\n",
    "        rmse = self.excute_RMSE(input, target)\n",
    "        return rmse > self.thr\n",
    "\n",
    "    def set_thr(self, thr):\n",
    "        self.thr = thr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6905b6-95b0-4570-ae00-3a10631f4c17",
   "metadata": {
    "id": "cf6905b6-95b0-4570-ae00-3a10631f4c17"
   },
   "source": [
    "# Train iGuard using Magnifier on 105 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6OlOeCNMPF0X",
   "metadata": {
    "id": "6OlOeCNMPF0X"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data, left=None, right=None, depth=0):\n",
    "        self.data = data\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "        self.label = None\n",
    "        self.val = None\n",
    "        self.col = None\n",
    "\n",
    "class isolationTree:\n",
    "    def __init__(self, data, depth=0, max_depth=0, thres = None):\n",
    "        self.max_depth = max_depth\n",
    "        self.thres = thres\n",
    "        self.root = self.build_tree(data, depth)\n",
    "        self.model = None\n",
    "        self.num_rows = 50\n",
    "\n",
    "    def build_tree(self, data, depth):\n",
    "        if len(data) == 0:\n",
    "            return None\n",
    "        elif len(data) == 1:\n",
    "            return Node(data, depth=depth)\n",
    "        else:\n",
    "            q = Queue()\n",
    "            root = Node(data, depth=depth)\n",
    "            q.put(root)\n",
    "\n",
    "            while not q.empty():\n",
    "                current_node = q.get()\n",
    "                if len(current_node.data) > 1:\n",
    "                  split_column = random.randint(0, len(current_node.data.iloc[0]) - 1)\n",
    "                  while True:\n",
    "                    if min(current_node.data.iloc[:, split_column]) != max(current_node.data.iloc[:, split_column]):\n",
    "                      break\n",
    "                    split_column = (split_column + 1) % len(current_node.data.iloc[0])\n",
    "                  split_value = random.uniform(min(current_node.data.iloc[:, split_column]), max(current_node.data.iloc[:, split_column]))\n",
    "                  left_data = current_node.data[current_node.data.iloc[:, split_column] <= split_value]\n",
    "                  right_data = current_node.data[current_node.data.iloc[:, split_column] > split_value]\n",
    "                  current_node.val = split_value\n",
    "                  current_node.col = split_column\n",
    "\n",
    "                  if len(left_data) > 0:\n",
    "                      left_node = Node(left_data, depth=current_node.depth + 1)\n",
    "                      current_node.left = left_node\n",
    "                      if len(left_data) != 1:\n",
    "                        q.put(left_node)\n",
    "\n",
    "                  if len(right_data) > 0:\n",
    "                      right_node = Node(right_data, depth=current_node.depth + 1)\n",
    "                      current_node.right = right_node\n",
    "                      if len(right_data) != 1:\n",
    "                        q.put(right_node)\n",
    "            return root\n",
    "\n",
    "    def label_tree(self, root):\n",
    "\n",
    "        if root.left is None and root.right is None:\n",
    "            sampled_df = root.data.sample(n=self.num_rows, replace=True, random_state=42)\n",
    "\n",
    "            noise = np.random.normal(0, 0.01, sampled_df.shape)\n",
    "\n",
    "            sampled_df = scaler.transform(root.data)\n",
    "            sampled_df = sampled_df + noise\n",
    "            sampled_df = np.pad(sampled_df, ((0,0),(3,0)), 'constant')\n",
    "\n",
    "            index = np.empty((0,0))\n",
    "            Port_index = np.arange(4,-1,-1).reshape(5,-1)\n",
    "            MIstat_index = np.arange(5,20).reshape(5,-1)\n",
    "            HHstat_index = np.arange(20,55).reshape(5,-1)\n",
    "            HHstat_jit_index = np.arange(55,70).reshape(5,-1)\n",
    "            HpHpstat_index = np.arange(70,105).reshape(5,-1)\n",
    "            for i in range(5):\n",
    "                index = np.append(index, Port_index[i])\n",
    "                index = np.append(index, MIstat_index[i])\n",
    "                index = np.append(index, HHstat_index[i])\n",
    "                index = np.append(index, HHstat_jit_index[i])\n",
    "                index = np.append(index, HpHpstat_index[i])\n",
    "            sampled_df = sampled_df[:, index.astype(int).tolist()].reshape(-1, 5, 21)\n",
    "            sampled_df = torch.tensor(sampled_df, dtype=torch.float32)\n",
    "            pred, _ = self.model(sampled_df)\n",
    "            rmse = criterion(pred, sampled_df)\n",
    "            rmse = torch.sqrt(rmse)\n",
    "            root.label = 1 if rmse > self.thres else 0\n",
    "        else:\n",
    "            if root.left:\n",
    "              self.label_tree(root.left)\n",
    "            if root.right:\n",
    "              self.label_tree(root.right)\n",
    "\n",
    "    def pred_tree(self, data, node):\n",
    "      if node is None:\n",
    "        return 1, 0\n",
    "\n",
    "      if node.left is None and node.right is None:\n",
    "        return node.label, 0\n",
    "      if data[node.col] <= node.val:\n",
    "        label, path_len = self.pred_tree(data,node.left)\n",
    "        return label, 1 + path_len\n",
    "      if data[node.col] > node.val:\n",
    "        label, path_len = self.pred_tree(data,node.right)\n",
    "        return label, 1 + path_len\n",
    "\n",
    "class isolationForest(nn.Module):\n",
    "    def __init__(self, data, n_trees=100, max_depth=5, subspace=256, model=None):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.subspace = subspace\n",
    "        self.model = model\n",
    "        self.thres = 0.4\n",
    "        self.avg_path_len = 2 * (np.log(subspace - 1) + 0.5772) - 2 * (subspace - 1) / subspace\n",
    "        self.trees = []\n",
    "    def forward(self, x):\n",
    "      return None\n",
    "    def fit(self):\n",
    "\n",
    "        for i in range(self.n_trees):\n",
    "            if self.subspace > 1:\n",
    "                subdata = self.data.sample(self.subspace)\n",
    "            else:\n",
    "                subdata = self.data.sample(frac=self.subspace)\n",
    "            tree = isolationTree(subdata, depth=0, max_depth = self.max_depth, thres = self.thres)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def label(self, model):\n",
    "        for tree in self.trees:\n",
    "            tree.model = model\n",
    "        for tree in self.trees:\n",
    "            tree.label_tree(tree.root)\n",
    "\n",
    "    def pred(self, data, alpha):\n",
    "      eval = []\n",
    "      for i,row in data.iterrows():\n",
    "          temp = [tree.pred_tree(row, tree.root) for tree in self.trees]\n",
    "          res = min(temp)[1], max(temp)[1]\n",
    "          print(res)\n",
    "          agg_label = sum(x[0] for x in temp) / self.n_trees\n",
    "          avg_len = sum(x[1] for x in temp) / self.n_trees\n",
    "\n",
    "          iso_score = 2 ** (- avg_len / self.avg_path_len)\n",
    "          print(f\"len: {avg_len} score: {iso_score}\")\n",
    "          eval.append(-1 if alpha * agg_label + (1 - alpha) * iso_score > 0.5 else 1)\n",
    "      return eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RVlh-t2VPmDV",
   "metadata": {
    "id": "RVlh-t2VPmDV"
   },
   "outputs": [],
   "source": [
    "def pred(model, data, alpha):\n",
    "  eval = []\n",
    "  for i,row in data.iterrows():\n",
    "      temp = [tree.pred_tree(row, tree.root) for tree in model.trees]\n",
    "      res = min(temp)[1], max(temp)[1]\n",
    "      agg_label = sum(x[0] for x in temp) / model.n_trees\n",
    "      avg_len = sum(x[1] for x in temp) / model.n_trees\n",
    "\n",
    "      iso_score = 1 - 2 ** (- avg_len / model.avg_path_len - np.log(1-0.1475) / np.log(2))\n",
    "      # print(f\"len: {avg_len} score: {iso_score}\")\n",
    "      eval.append(-1 if alpha * agg_label + (1 - alpha) * iso_score > 0.5 else 1)\n",
    "  return eval\n",
    "\n",
    "def iGuard_control_plane(auto):\n",
    "    tmp = pd.concat([x_train,attack_x_train])\n",
    "    tmp.fillna(0,inplace=True)\n",
    "    tmp = tmp.iloc[:, 1:]\n",
    "    tmp.columns = range(len(tmp.columns))\n",
    "    clf_model = isolationForest(tmp, n_trees=50, max_depth=8, subspace=200)\n",
    "    clf_model.fit()\n",
    "    print(\"Isolation Forest training completed\\n\")\n",
    "    auto = auto.cpu()\n",
    "    clf_model.label(auto)\n",
    "    print(\"Isolation Forest labeling completed\\n\")\n",
    "    x_eval = eval\n",
    "    x_eval.fillna(0,inplace=True)\n",
    "    x_eval = x_eval.iloc[:, 1:]\n",
    "    x_eval.columns = range(len(x_eval.columns))\n",
    "    y_pred_eval = pred(clf_model, x_eval, 1)\n",
    "    eval_y = y_eval\n",
    "    eval_x = x_eval\n",
    "\n",
    "    y_pred_eval = np.array(y_pred_eval)\n",
    "    y_pred_eval[y_pred_eval == 1] = 0\n",
    "    y_pred_eval[y_pred_eval == -1] = 1\n",
    "    temp_str = classification_report(y_true=eval_y, y_pred=y_pred_eval)\n",
    "    temp_list = temp_str.split()\n",
    "    print(\"Our Isolation Forest + Autoencoder\")\n",
    "    print(temp_str)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(eval_y, y_pred_eval))\n",
    "    print(\"\\n F1 Score\")\n",
    "    print(f1_score(eval_y, y_pred_eval))\n",
    "    print(\"\\n roc-auc\")\n",
    "    print(roc_auc_score(eval_y, y_pred_eval))\n",
    "    print(\"\\n pr-auc\")\n",
    "    print(average_precision_score(eval_y, y_pred_eval))\n",
    "    return clf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4Vm0qFK2QNNj",
   "metadata": {
    "id": "4Vm0qFK2QNNj"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "TWO_D = True\n",
    "INPUTSIZE = 105\n",
    "TEST_BATCH_SIZE = 60000\n",
    "\n",
    "def train_data_processing(df_normal_train, df_attack_eval, x_eval, y_eval, df_normal_eval, TWO_D):\n",
    "    X_train, y_train= df_normal_train.drop(columns=[0, 'class']), df_normal_train['class']\n",
    "    X_normal_eval = df_normal_eval.drop(columns=[0, 'class'])\n",
    "    X_attack_eval = df_attack_eval.drop(columns=[0, 'class'])\n",
    "    X_train, y_train = X_train.values, y_train.values\n",
    "    X_normal_eval = X_normal_eval.values\n",
    "    X_attack_eval = X_attack_eval.values\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_normal_eval = scaler.transform(X_normal_eval)\n",
    "    X_attack_eval = scaler.transform(X_attack_eval)\n",
    "    x_eval = x_eval.drop(columns=[0])\n",
    "    x_eval = scaler.transform(x_eval)\n",
    "    y_eval = y_eval.values\n",
    "\n",
    "    if TWO_D:\n",
    "        # Padding\n",
    "        X_train = np.pad(X_train, ((0,0),(3,0)), 'constant')\n",
    "        x_eval = np.pad(x_eval, ((0,0),(3,0)), 'constant')\n",
    "        X_normal_eval = np.pad(X_normal_eval, ((0,0),(3,0)), 'constant')\n",
    "        X_attack_eval = np.pad(X_attack_eval, ((0,0),(3,0)), 'constant')\n",
    "        index = np.empty((0,0))\n",
    "        Port_index = np.arange(4,-1,-1).reshape(5,-1)\n",
    "        MIstat_index = np.arange(5,20).reshape(5,-1)\n",
    "        HHstat_index = np.arange(20,55).reshape(5,-1)\n",
    "        HHstat_jit_index = np.arange(55,70).reshape(5,-1)\n",
    "        HpHpstat_index = np.arange(70,105).reshape(5,-1)\n",
    "        for i in range(5):\n",
    "            index = np.append(index, Port_index[i])\n",
    "            index = np.append(index, MIstat_index[i])\n",
    "            index = np.append(index, HHstat_index[i])\n",
    "            index = np.append(index, HHstat_jit_index[i])\n",
    "            index = np.append(index, HpHpstat_index[i])\n",
    "        X_train = X_train[:, index.astype(int).tolist()].reshape(-1, 5, 21)\n",
    "        x_eval = x_eval[:, index.astype(int).tolist()].reshape(-1, 5, 21)\n",
    "        X_normal_eval = X_normal_eval[:, index.astype(int).tolist()].reshape(-1, 5, 21)\n",
    "        X_attack_eval = X_attack_eval[:, index.astype(int).tolist()].reshape(-1, 5, 21)\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    x_eval = torch.tensor(x_eval, dtype=torch.float32)\n",
    "    X_normal_eval = torch.tensor(X_normal_eval, dtype=torch.float32)\n",
    "    X_attack_eval = torch.tensor(X_attack_eval, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    y_eval = torch.tensor(y_eval)\n",
    "\n",
    "    return X_train, y_train, x_eval, y_eval, X_normal_eval, X_attack_eval\n",
    "\n",
    "\n",
    "def train_magnifier(model, df_normal_train, df_attack_eval, model_save_path, x_eval, y_eval,  df_normal_eval, TWO_D):\n",
    "    lr = 1e-2\n",
    "    weight_decay = 0.01\n",
    "    epoches = 5\n",
    "    INPUTSIZE = 100\n",
    "\n",
    "    X_train, y_train, x_eval, y_eval, X_normal_eval, X_attack_eval = train_data_processing(df_normal_train, df_attack_eval, x_eval, y_eval, df_normal_eval, TWO_D)\n",
    "    train_datasets = Data.TensorDataset(X_train, y_train)\n",
    "    train_loader = Data.DataLoader(dataset=train_datasets, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    test_datasets = Data.TensorDataset(x_eval, y_eval)\n",
    "    test_loader = Data.DataLoader(dataset=test_datasets, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "    print(\"Total number of Epoch: \", epoches)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        if epoch in [epoches * 0.5, epoches * 1.0]:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, (b_x, b_y) in enumerate(train_loader):\n",
    "            # forward\n",
    "            if torch.cuda.is_available():\n",
    "              b_x = b_x.cuda()\n",
    "            output, encoded = model(b_x)\n",
    "            loss = criterion(b_x, output)\n",
    "            loss = torch.sqrt(loss)\n",
    "            total_loss += loss.sum()\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            loss_normal = 0\n",
    "            for i in range(0, X_normal_eval.shape[0], 16):\n",
    "                if torch.cuda.is_available():\n",
    "                  b_x = X_normal_eval[i:i + 16].cuda()\n",
    "                else:\n",
    "                  b_x = X_normal_eval[i:i + 16]\n",
    "                temp_x, _ = model(b_x)\n",
    "                loss_temp = criterion(temp_x, b_x)\n",
    "                loss_normal += loss_temp\n",
    "            loss_normal = (loss_normal.detach().cpu().numpy()) / i\n",
    "            print('the eval normal loss ', loss_normal)\n",
    "            loss_attack = 0\n",
    "            for i in range(0, X_attack_eval.shape[0], 16):\n",
    "                if torch.cuda.is_available():\n",
    "                  b_x = X_attack_eval[i:i + 16].cuda()\n",
    "                temp_x, _ = model(b_x)\n",
    "                loss_temp = criterion(temp_x, b_x)\n",
    "                loss_attack += loss_temp\n",
    "            loss_attack = (loss_attack.detach().cpu().numpy()) / i\n",
    "            print('the eval attack loss ', loss_attack)\n",
    "\n",
    "        print(\"epoch=\", epoch, total_loss)\n",
    "        print(len(X_train))\n",
    "    test_magnifier(model, test_loader, y_eval)\n",
    "    return model\n",
    "\n",
    "def test_magnifier(test_model, test_loader, y_eval):\n",
    "    begin = time.time()\n",
    "    test_model.thr = 0.4\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_model.eval()\n",
    "        correct = 0.\n",
    "        df_score = []\n",
    "        rmse_list = []\n",
    "        y_pred_eval = []\n",
    "        eval_y = []\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            if torch.cuda.is_available():\n",
    "              data = data.cuda()\n",
    "            eval_output, _ = test_model(data)\n",
    "            y_pred_eval.extend(test_model.pred(eval_output, data))\n",
    "            eval_y.extend(target)\n",
    "\n",
    "    temp_str = classification_report(y_true=eval_y, y_pred=y_pred_eval)\n",
    "    temp_list = temp_str.split()\n",
    "    print(\"Autoencoder\")\n",
    "    print(temp_str)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(eval_y, y_pred_eval))\n",
    "    print(\"\\n F1 Score\")\n",
    "    print(f1_score(eval_y, y_pred_eval))\n",
    "    print(\"\\n roc-auc\")\n",
    "    print(roc_auc_score(eval_y, y_pred_eval))\n",
    "    print(\"\\n pr-auc\")\n",
    "    print(average_precision_score(eval_y, y_pred_eval))\n",
    "\n",
    "def magnifier_trainer():\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    model = Magnifier(input_size=INPUTSIZE, seed=42)\n",
    "    magnifier = train_magnifier(model, df_normal_train, df_attack_eval, model_save_path, x_eval, y_eval, df_normal_eval, TWO_D)\n",
    "    print(\"Autoencoder training completed\\n\")\n",
    "    return magnifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Imx2Eh6QWVLC",
   "metadata": {
    "id": "Imx2Eh6QWVLC"
   },
   "outputs": [],
   "source": [
    "def sk_iforest():\n",
    "    sklearn_clf_model = IsolationForest(n_estimators=50, max_samples=200, random_state=114514,\n",
    "                             contamination=0.2,n_jobs=8)\n",
    "\n",
    "    tmp = pd.concat([x_train,attack_x_train])\n",
    "    tmp.fillna(0,inplace=True)\n",
    "    sklearn_clf_model.fit(tmp.iloc[:, 1:])\n",
    "\n",
    "    y_pred_eval = sklearn_clf_model.predict(x_eval.iloc[:, 1:])\n",
    "    eval_y = y_eval\n",
    "    eval_x = x_eval\n",
    "\n",
    "    y_pred_eval[y_pred_eval == 1] = 0\n",
    "    y_pred_eval[y_pred_eval == -1] = 1\n",
    "    temp_str = classification_report(y_true=eval_y, y_pred=y_pred_eval)\n",
    "    temp_list = temp_str.split()\n",
    "    print(\"sklearn Isolation Forest\")\n",
    "    print(temp_str)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(eval_y, y_pred_eval))\n",
    "    print(\"\\n F1 Score\")\n",
    "    print(f1_score(eval_y, y_pred_eval))\n",
    "    print(\"\\n roc-auc\")\n",
    "    print(roc_auc_score(eval_y, y_pred_eval))\n",
    "    print(\"\\n pr-auc\")\n",
    "    print(average_precision_score(eval_y, y_pred_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0816221-5f69-478d-982c-24445194a4f9",
   "metadata": {
    "id": "c0816221-5f69-478d-982c-24445194a4f9"
   },
   "source": [
    "## Experiment 2: Fig. 5 and Appendix Fig. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db1772-5994-40ce-86d9-bf7e9171ea71",
   "metadata": {
    "id": "96db1772-5994-40ce-86d9-bf7e9171ea71"
   },
   "outputs": [],
   "source": [
    "# Use iGuard_control_plane to obtain comparison with Magnifier and iForest on F1 score, ROC_AUC and ROC_PR on best configuration.\n",
    "# Refer to your google doc\n",
    "\n",
    "device_list=['360_camera']\n",
    "attack_list=['http_ddos','data_theft','keylogging','service_scan','tcp_ddos','mirai','os_scan','aidra','bashlite','mirai_router_filter','os_scan_router','port_scan_router','tcp_ddos_router','udp_ddos','udp_ddos_router']\n",
    "\n",
    "df_normal_train_data = load_iot_data_seq(device_list=device_list, begin=0, end=1)\n",
    "contamination = 0.4\n",
    "for a in attack_list:\n",
    "\n",
    "    df_attack_train = load_iot_attack_seq(a)\n",
    "    if contamination != -1:\n",
    "      num = int((1000 / contamination - 1000) / 0.2)\n",
    "      df_normal_train_data = df_normal_train_data.sample(n=num, replace=False, random_state=20)\n",
    "      df_attack_train = df_attack_train.sample(n=5000, replace=False, random_state=20)\n",
    "    df_normal_train, df_normal_eval = train_test_split(df_normal_train_data, test_size=0.2, random_state=20)\n",
    "\n",
    "    df_attack_train, df_attack_eval = train_test_split(df_attack_train, test_size=0.2, random_state=20)\n",
    "    x_train, y_train = df_normal_train.drop(columns=['class']), df_normal_train['class']\n",
    "    attack_x_train, attack_y_train = df_attack_train.drop(columns=['class']), df_attack_train['class']\n",
    "    df_eval = pd.concat([df_normal_eval, df_attack_eval])\n",
    "    x_eval, y_eval = df_eval.drop(columns=['class']), df_eval['class']\n",
    "    eval = x_eval\n",
    "    print(\"------------------------------Attack : \",a, \"--------------------------------\\n\")\n",
    "    sk_iforest()\n",
    "    magnifier = magnifier_trainer()\n",
    "    iGuard = iGuard_control_plane(magnifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c5709-dc53-4115-8c05-d54d63f5cf7b",
   "metadata": {
    "id": "108c5709-dc53-4115-8c05-d54d63f5cf7b"
   },
   "source": [
    "# Train iGuard on data plane features using custom autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Iz7wKvyKciDz",
   "metadata": {
    "id": "Iz7wKvyKciDz"
   },
   "outputs": [],
   "source": [
    "def sk_iforest(a):\n",
    "    sklearn_clf_model = IsolationForest(n_estimators=50, max_samples=200, random_state=114514,\n",
    "                             contamination=0.2,n_jobs=8)\n",
    "    tmp = pd.concat([x_train,attack_x_train])\n",
    "    tmp.fillna(0,inplace=True)\n",
    "    sklearn_clf_model.fit(tmp)\n",
    "    y_pred_eval = sklearn_clf_model.predict(x_eval)\n",
    "    eval_y = y_eval\n",
    "    eval_x = x_eval\n",
    "\n",
    "    y_pred_eval[y_pred_eval == 1] = 0\n",
    "    y_pred_eval[y_pred_eval == -1] = 1\n",
    "    temp_str = classification_report(y_true=eval_y, y_pred=y_pred_eval)\n",
    "    temp_list = temp_str.split()\n",
    "    print(\"sklearn Isolation Forest\")\n",
    "    print(temp_str)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(eval_y, y_pred_eval))\n",
    "    print(\"\\n F1 Score\")\n",
    "    print(f1_score(eval_y, y_pred_eval))\n",
    "    print(\"\\n roc-auc\")\n",
    "    print(roc_auc_score(eval_y, y_pred_eval))\n",
    "    print(\"\\n pr-auc\")\n",
    "    print(average_precision_score(eval_y, y_pred_eval))\n",
    "    name = 'iForest'+a+'.pkl'\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(sklearn_clf_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jpjAEhHobWjv",
   "metadata": {
    "id": "jpjAEhHobWjv"
   },
   "outputs": [],
   "source": [
    "class DilatedSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        super(DilatedSeparableConv, self).__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=dilation, dilation=dilation, groups=in_channels)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        self.encoder = nn.Sequential(\n",
    "            DilatedSeparableConv(1, 16, dilation=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(2),\n",
    "            DilatedSeparableConv(16, 32, dilation=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(2),\n",
    "            DilatedSeparableConv(32, 64, dilation=4),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, latent_dim, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(latent_dim, 64, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=5, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose1d(16, 1, kernel_size=6, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z3FVnUn7b3_A",
   "metadata": {
    "id": "Z3FVnUn7b3_A"
   },
   "outputs": [],
   "source": [
    "def train_custom_autoencoder(df_normal_train, df_normal_eval, df_attack_eval):\n",
    "    X_train = df_normal_train.values\n",
    "    X_test = pd.concat([df_normal_eval, df_attack_eval]).values\n",
    "    actual = torch.cat([torch.zeros(df_normal_eval.shape[0]), torch.ones(df_attack_eval.shape[0])])\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = 8\n",
    "    num_epochs = 1000\n",
    "    batch_size = 512\n",
    "\n",
    "    autoencoder = Autoencoder(input_dim, encoding_dim)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "    #optimizer = optim.Adagrad(autoencoder.parameters(), lr=0.01)\n",
    "    #optimizer = optim.RMSprop(autoencoder.parameters(), lr=0.001, alpha=0.9)\n",
    "    #optimizer = optim.SGD(autoencoder.parameters(), lr=0.1, weight_decay=0.001)\n",
    "\n",
    "    autoencoder = autoencoder.cuda()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            inputs = torch.tensor(X_train[i:i+batch_size], dtype=torch.float).unsqueeze(0)\n",
    "            inputs = inputs.cuda()\n",
    "            inputs = inputs.transpose(0, 1)\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            total_loss += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 200 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.sum():.4f}')\n",
    "\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        input = torch.tensor(X_test, dtype=torch.float).unsqueeze(0).transpose(0, 1).cuda()\n",
    "        decoded_data = autoencoder(input).cpu().numpy()\n",
    "    decoded_data_binary = decoded_data.squeeze()\n",
    "    mse = np.mean(np.power(X_test - decoded_data_binary, 2), axis=1)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    for i in [0.001]:\n",
    "        thres = i\n",
    "        print(\"-------------i\",i)\n",
    "        predicted = pd.Series(np.where(rmse > thres, 1, 0),dtype=\"float64\")\n",
    "        f1 = f1_score(actual, predicted)\n",
    "        print(confusion_matrix(actual, predicted))\n",
    "        print(\"\\n Classification report\")\n",
    "        print(classification_report(actual, predicted))\n",
    "        print('F1 Score: ', f1)\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iE5mpBo1cc7T",
   "metadata": {
    "id": "iE5mpBo1cc7T"
   },
   "outputs": [],
   "source": [
    "def custom_autoencoder():\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    auto = train_custom_autoencoder(df_norm_train.iloc[:,:12], df_normal_eval.iloc[:,:12], df_attack_eval.iloc[:,:12])\n",
    "    print(\"Autoencoder training completed\\n\")\n",
    "    return auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9YmFTrWucUH7",
   "metadata": {
    "id": "9YmFTrWucUH7"
   },
   "outputs": [],
   "source": [
    "def iGuard_data_plane(auto,a):\n",
    "    tmp = pd.concat([x_train,attack_x_train])\n",
    "    tmp.fillna(0,inplace=True)\n",
    "    tmp.columns = range(len(tmp.columns))\n",
    "    clf_model = isolationForest(tmp, n_trees=50, max_depth=8, subspace=200)\n",
    "    clf_model.fit()\n",
    "    print(\"Isolation Forest training completed\\n\")\n",
    "    auto = auto.cpu()\n",
    "    clf_model.label(auto)\n",
    "    print(\"Isolation Forest labeling completed\\n\")\n",
    "    x_eval = eval\n",
    "    x_eval.fillna(0,inplace=True)\n",
    "    x_eval.columns = range(len(x_eval.columns))\n",
    "    y_pred_eval = pred(clf_model, x_eval, 1)\n",
    "    eval_y = y_eval\n",
    "    eval_x = x_eval\n",
    "\n",
    "    y_pred_eval = np.array(y_pred_eval)\n",
    "    y_pred_eval[y_pred_eval == 1] = 0\n",
    "    y_pred_eval[y_pred_eval == -1] = 1\n",
    "    print(\"Our Isolation Forest + Autoencoder\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(eval_y, y_pred_eval))\n",
    "    print(\"\\n Classification report\")\n",
    "    print(classification_report(eval_y, y_pred_eval))\n",
    "    print(\"\\n F1 Score\")\n",
    "    print(f1_score(eval_y, y_pred_eval))\n",
    "    print(\"\\n roc-auc\")\n",
    "    print(roc_auc_score(eval_y, y_pred_eval))\n",
    "    print(\"\\n pr-auc\")\n",
    "    print(average_precision_score(eval_y, y_pred_eval))\n",
    "    name = 'iGuard'+a+'.pkl'\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(clf_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E2uD_K2kcO2h",
   "metadata": {
    "id": "E2uD_K2kcO2h"
   },
   "outputs": [],
   "source": [
    "device_list=['360_camera']\n",
    "attack_list=['http_ddos','data_theft','keylogging','service_scan','tcp_ddos','mirai','os_scan','aidra','bashlite','mirai_router_filter','os_scan_router','port_scan_router','tcp_ddos_router','udp_ddos','udp_ddos_router']\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "contamination = 0.2\n",
    "for a in attack_list:\n",
    "\n",
    "    normal_path = \"/content/drive/MyDrive/HorusEye/DataSets/Dataplane/Normal/\" + device_list[0] + \".csv\"\n",
    "    attack_path = \"/content/drive/MyDrive/HorusEye/DataSets/Dataplane/Anomaly/\" + a + \".csv\"\n",
    "    df_normal_train_data = pd.read_csv(normal_path)\n",
    "    df_attack_train = pd.read_csv(attack_path)\n",
    "    df_normal_train_data = df_normal_train_data.drop(columns=['tcp_udp','dst_port','n_packets'])\n",
    "    df_attack_train = df_attack_train.drop(columns=['tcp_udp','dst_port','n_packets'])\n",
    "    df_normal_train_data = df_normal_train_data.applymap(int)\n",
    "    df_attack_train = df_attack_train.applymap(int)\n",
    "    datafetch = True\n",
    "    n1 = 200\n",
    "    n2 = 1000\n",
    "    while(datafetch):\n",
    "        if contamination != -1:\n",
    "            try:\n",
    "                num = int((n1 / contamination - n1) / 0.2)\n",
    "                temp = df_normal_train_data.sample(n=num, replace=False, random_state=20)\n",
    "                df_norm_train = df_normal_train_data.drop(temp.index)\n",
    "\n",
    "                df_attack_train = df_attack_train.sample(n=n2, replace=False, random_state=20)\n",
    "                df_normal_train_data = temp\n",
    "                datafetch = False\n",
    "            except ValueError as e:\n",
    "                if \"Cannot take a larger sample than population when 'replace=False'\" in str(e):\n",
    "                    n1 = int(n1/2)\n",
    "                    n2 = int(n2/2)\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "    df_normal_train, df_normal_eval = train_test_split(df_normal_train_data, test_size=0.2, random_state=20)\n",
    "    df_norm_train = pd.concat([df_norm_train, df_normal_train])\n",
    "    df_attack_train, df_attack_eval = train_test_split(df_attack_train, test_size=0.2, random_state=20)\n",
    "    x_train, y_train = df_normal_train.drop(columns=['class']), df_normal_train['class']\n",
    "    attack_x_train, attack_y_train = df_attack_train.drop(columns=['class']), df_attack_train['class']\n",
    "    df_eval = pd.concat([df_normal_eval, df_attack_eval])\n",
    "    x_eval, y_eval = df_eval.drop(columns=['class']), df_eval['class']\n",
    "    eval = x_eval\n",
    "    print(\"------------------------------Attack : \",a, \"--------------------------------\\n\")\n",
    "    sk_iforest(a)\n",
    "    auto = custom_autoencoder()\n",
    "    iGuard_data_plane(auto,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c3b79-c436-4399-a5d1-72960bc4b49f",
   "metadata": {
    "id": "946c3b79-c436-4399-a5d1-72960bc4b49f"
   },
   "source": [
    "## Experiment 3: Fig. 6, Appendix Fig. 9, and Tables 2, 3 (Data Plane Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d08f5-2c71-41d8-b053-67a3c9701dd9",
   "metadata": {
    "id": "797d08f5-2c71-41d8-b053-67a3c9701dd9"
   },
   "outputs": [],
   "source": [
    "# Use iGuard_data_plane to obtain comparison with Magnifier and iForest on F1 score, ROC_AUC and ROC_PR on best configuration.\n",
    "# Refer to your google doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c48f9-1049-4b8a-af29-a3b4640e681c",
   "metadata": {
    "id": "7d7c48f9-1049-4b8a-af29-a3b4640e681c"
   },
   "source": [
    "## Experiment 4: Comparison with VAE (See Fig. 10 in Appendix). Compare Magnifier with VAE would suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81112bc-bc34-4ed0-8ab6-4932adcda009",
   "metadata": {
    "id": "b81112bc-bc34-4ed0-8ab6-4932adcda009"
   },
   "outputs": [],
   "source": [
    "# Define the VAE model with 4 hidden layers\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        self.fc21 = nn.Linear(hidden_dim4, latent_dim)  # Mean\n",
    "        self.fc22 = nn.Linear(hidden_dim4, latent_dim)  # Log variance\n",
    "\n",
    "        # Decoder\n",
    "        self.fc5 = nn.Linear(latent_dim, hidden_dim4)\n",
    "        self.fc6 = nn.Linear(hidden_dim4, hidden_dim3)\n",
    "        self.fc7 = nn.Linear(hidden_dim3, hidden_dim2)\n",
    "        self.fc8 = nn.Linear(hidden_dim2, hidden_dim1)\n",
    "        self.fc9 = nn.Linear(hidden_dim1, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        h2 = torch.relu(self.fc2(h1))\n",
    "        h3 = torch.relu(self.fc3(h2))\n",
    "        h4 = torch.relu(self.fc4(h3))\n",
    "        return self.fc21(h4), self.fc22(h4)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h5 = torch.relu(self.fc5(z))\n",
    "        h6 = torch.relu(self.fc6(h5))\n",
    "        h7 = torch.relu(self.fc7(h6))\n",
    "        h8 = torch.relu(self.fc8(h7))\n",
    "        return torch.sigmoid(self.fc9(h8))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    def pred(self, x, threshold):\n",
    "        recon, _, _ = self.forward(x)\n",
    "        mse_loss = F.mse_loss(recon, x, reduction='none')\n",
    "        mse_per_sample = mse_loss.mean(dim=1)\n",
    "        predictions = (mse_per_sample > threshold).float()  # Convert boolean to float (0 or 1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 103\n",
    "hidden_dim1 = 128\n",
    "hidden_dim2 = 64\n",
    "hidden_dim3 = 32\n",
    "hidden_dim4 = 16\n",
    "latent_dim = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12164d-8555-4d7c-a130-325a7ac6dc74",
   "metadata": {
    "id": "ca12164d-8555-4d7c-a130-325a7ac6dc74"
   },
   "outputs": [],
   "source": [
    "device_list=['360_camera']\n",
    "attack_list=['http_ddos','data_theft','keylogging','service_scan','tcp_ddos','mirai','os_scan','aidra','bashlite','mirai_router_filter','os_scan_router','port_scan_router','tcp_ddos_router','udp_ddos','udp_ddos_router']\n",
    "\n",
    "for a in attack_list:\n",
    "    df_normal_train_data = load_iot_data_seq(device_list=device_list, begin=0, end=1)\n",
    "    df_attack_train = load_iot_attack_seq(a)\n",
    "\n",
    "    df_normal_train, df_normal_eval = train_test_split(df_normal_train_data, test_size=0.2, random_state=20)\n",
    "    df_attack_train, df_attack_eval = train_test_split(df_attack_train, test_size=0.2, random_state=20)\n",
    "\n",
    "    df_eval = pd.concat([df_normal_eval,df_attack_eval], ignore_index=True)\n",
    "\n",
    "    X_train, y_train = df_normal_train.drop(columns=['class']), df_normal_train['class']\n",
    "    X_attack_test, y_attack_test = df_attack_train.drop(columns=['class']), df_attack_train['class']\n",
    "    X_normal_test, y_noraml_test = df_normal_eval.drop(columns=['class']), df_normal_eval['class']\n",
    "    df_X_test, df_y_test = df_eval.drop(columns=['class']), df_eval['class']\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train.values)\n",
    "    X_attack_test = scaler.transform(X_attack_test.values)\n",
    "    X_normal_test = scaler.transform(X_normal_test.values)\n",
    "    df_X_test = scaler.transform(df_X_test.values)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "    X_attack_test = torch.tensor(X_attack_test, dtype=torch.float32)\n",
    "    y_attack_test = torch.tensor(y_attack_test.values, dtype=torch.float32)\n",
    "    X_normal_test = torch.tensor(X_normal_test, dtype=torch.float32)\n",
    "    y_noraml_test = torch.tensor(y_noraml_test.values, dtype=torch.float32)\n",
    "    df_X_test = torch.tensor(df_X_test, dtype=torch.float32)\n",
    "    df_y_test = torch.tensor(df_y_test.values, dtype=torch.float32)\n",
    "\n",
    "    train_datasets = TensorDataset(X_train)\n",
    "    train_loader = DataLoader(dataset=train_datasets, batch_size=256, shuffle=True, num_workers=0)\n",
    "    normal_test_datasets = TensorDataset(X_normal_test, y_noraml_test)\n",
    "    normal_test_loader = DataLoader(dataset=normal_test_datasets, batch_size=256, shuffle=True, num_workers=0)\n",
    "    attack_test_datasets = TensorDataset(X_attack_test, y_attack_test)\n",
    "    attack_test_loader = DataLoader(dataset=attack_test_datasets, batch_size=256, shuffle=True, num_workers=0)\n",
    "    test_datasets = TensorDataset(df_X_test, df_y_test)\n",
    "    test_loader = DataLoader(dataset=test_datasets, batch_size=256, shuffle=True, num_workers=0)\n",
    "    print(\"------------------------------Attack : \",a, \"--------------------------------\\n\")\n",
    "\n",
    "    model = VAE(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, latent_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (x,) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(x)\n",
    "            loss = loss_function(recon_batch, x, mu, logvar)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {total_loss}')\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_normal = 0\n",
    "        for batch_idx, (x, y) in enumerate(normal_test_loader):\n",
    "            b_x = x\n",
    "            temp_x, _, _ = model(b_x)\n",
    "            loss_temp = criterion(temp_x, b_x)\n",
    "            loss_normal += loss_temp\n",
    "        loss_normal = (loss_normal.detach().cpu().numpy())\n",
    "        print('the eval normal loss ', loss_normal)\n",
    "        loss_attack = 0\n",
    "        for batch_idx, (x, y) in enumerate(attack_test_loader):\n",
    "            b_x = x\n",
    "            temp_x, _, _ = model(b_x)\n",
    "            loss_temp = criterion(temp_x, b_x)\n",
    "            loss_attack += loss_temp\n",
    "        loss_attack = (loss_attack.detach().cpu().numpy())\n",
    "\n",
    "        print('the eval attack loss ', loss_attack)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0.\n",
    "        df_score = []\n",
    "        rmse_list = []\n",
    "        y_pred_eval = []\n",
    "        eval_y = []\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            y_pred_eval.extend(model.pred(data, 0.1))\n",
    "            eval_y.extend(target)\n",
    "\n",
    "    temp_str = classification_report(y_true=eval_y, y_pred=y_pred_eval)\n",
    "    temp_list = temp_str.split()\n",
    "    print(\"VAE\")\n",
    "    print(temp_str)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(eval_y, y_pred_eval))\n",
    "    print(\"\\n F1 Score\")\n",
    "    print(f1_score(eval_y, y_pred_eval))\n",
    "    print(\"\\n roc-auc\")\n",
    "    print(roc_auc_score(eval_y, y_pred_eval))\n",
    "    print(\"\\n pr-auc\")\n",
    "    print(average_precision_score(eval_y, y_pred_eval))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
